{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN32V0QYiqt4Pnhd10WwRUh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wezz-git/AI-samples/blob/main/(NLP_TF_IDF)_Bag_of_Words_Spam_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Business Problem: **\n",
        "\n",
        "You work for a messaging company (like WhatsApp or a mobile carrier). The system handles millions of messages per second. You need a model to automatically flag messages as \"Spam\" or \"Ham\" (legitimate).\n",
        "\n",
        "**Constraint:** You cannot use a massive Deep Learning model (like FinBERT) because it’s too slow and expensive to run on millions of messages. You need something fast, lightweight, and accurate.\n",
        "\n",
        "This is Classical NLP using **TF-IDF** (Term Frequency-Inverse Document Frequency). Instead of trying to \"understand\" the meaning of words (like Deep Learning), we turn text into a mathematical spreadsheet based on word counts.\n",
        "\n",
        "If a message contains \"free\", \"winner\", and \"cash,\" it’s likely spam.\n",
        "\n",
        "**TF-IDF** calculates a score for every word based on how rare or important it is.\n",
        "\n",
        "**The Model:** Use of Naive Bayes (or Logistic Regression). These are for text classification—simple, incredibly fast, and surprisingly accurate for text."
      ],
      "metadata": {
        "id": "R3c9Ki5zF0sB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1Y2A2i8EFd4M"
      },
      "outputs": [],
      "source": [
        "# Data Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# NLP Tools\n",
        "# TfidVectorizer - Turns text into numbere based o word importance\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# ML models\n",
        "# MultinomialNB (Naive Bayes) - Standard for basic text Classifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load & Clean:**\n",
        "\n",
        "1- Dataset is 'messy'. It often has Encoding Issues (it's not standard text format). We need to tell pandas to read it as 'latin-1'.\n",
        "\n",
        "2- It contains junk columns that needs dropping"
      ],
      "metadata": {
        "id": "MiBxepPoAYjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 - Load data with specific encoding\n",
        "# Use encoding='latin-1', if not then the file crashes pandas\n",
        "\n",
        "df = pd.read_csv('/content/sample_data/spam.csv', encoding='latin-1')\n",
        "\n",
        "# 2 - Drop the 'junk' columns (Unnamed: 2,3,4)\n",
        "# Only keep first 2 columns\n",
        "df = df[['v1','v2']]\n",
        "\n",
        "# 3 - Rename the columns (v1 is 'label', v2 is 'message')\n",
        "df.rename(columns={'v1':'label','v2':'message'}, inplace=True)\n",
        "\n",
        "# 4 - Check clean data\n",
        "print(\"--- Spam Data Loaded ---\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\n--- Class Balance ---\")\n",
        "print(df['label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AicptTyG_d5D",
        "outputId": "b8a16e51-9475-4a00-e542-5e410b176839"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Spam Data Loaded ---\n",
            "  label                                            message\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
            "\n",
            "--- Class Balance ---\n",
            "label\n",
            "ham     4825\n",
            "spam     747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label Encoding & Splitting:\n",
        "\n",
        "1 - Convert Labels: Turn 'ham' into 0 and 'spam' into 1.\n",
        "\n",
        "2 - Split Data: Create your Train/Test sets."
      ],
      "metadata": {
        "id": "FLEhEKQlErkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import X\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1 - Convert labels to numbers manually\n",
        "# Create a new column 'label_num' where ham=0, spam=1\n",
        "df['label_num'] = df['label'].map({'ham':0, 'spam':1})\n",
        "\n",
        "# 2 - Split data into\n",
        "X = df['message']\n",
        "y = df['label_num']  # y is always the target\n",
        "\n",
        "# 3 - Split the data\n",
        "# we'll reserve 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['message'], df['label_num'], test_size=0.2, random_state=42)\n",
        "\n",
        "# 4 - Check the split\n",
        "print(f\"Training on {len(X_train)} messages\")\n",
        "print(f\"Testing on {len(X_test)} messages\")\n",
        "print(\"\\n--- Example Training Message ---\")\n",
        "print(f\"Label: {y_train.iloc[0]}\")\n",
        "print(f\"Message: {X_train.iloc[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aeQxPtjyE6xU",
        "outputId": "67f20e9f-e7b4-4ede-bd44-094a4caa1742"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 4457 messages\n",
            "Testing on 1115 messages\n",
            "\n",
            "--- Example Training Message ---\n",
            "Label: 0\n",
            "Message: No I'm in the same boat. Still here at my moms. Check me out on yo. I'm half naked.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use of TF-IDF (Term Frequency-Inverse Document Frequency\n",
        "\n",
        "It gives every word a \"score.\" Rare, spammy words get high scores. Common words get low scores."
      ],
      "metadata": {
        "id": "RDpRQqSyKE7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# 1 -  Vectorize the Text (Turn words into numbers)\n",
        "print(\"Vectorizing text...\")\n",
        "\n",
        "# Initialize the tool\n",
        "# 'stop_words - remove common useless words \"the\", \"is\", \"at\"\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Learn from train dats & transform it\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform test data (dont learn, just transform it)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"Vocabulary size: {len(vectorizer.get_feature_names_out())} unique words found.\")\n",
        "\n",
        "# 2 - train model\n",
        "print(\"Training Naive Bayes Model..\")\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "print(\"Training done!\")\n",
        "\n",
        "print(\"Making predictions...\")\n",
        "predictions = model.predict(X_test_tfidf)\n",
        "\n",
        "print(\"Evaluating model...\")\n",
        "print(classification_report(y_test, predictions, target_names=['ham (legit)','spam']))\n",
        "\n",
        "# Get confusion matrix\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "print(confusion_matrix(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PVgOvfJOKEGb",
        "outputId": "cc03f985-227d-4035-8e8b-ce7c9fbb70b5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorizing text...\n",
            "Vocabulary size: 7472 unique words found.\n",
            "Training Naive Bayes Model..\n",
            "Training done!\n",
            "Making predictions...\n",
            "Evaluating model...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " ham (legit)       0.96      1.00      0.98       965\n",
            "        spam       1.00      0.75      0.86       150\n",
            "\n",
            "    accuracy                           0.97      1115\n",
            "   macro avg       0.98      0.88      0.92      1115\n",
            "weighted avg       0.97      0.97      0.96      1115\n",
            "\n",
            "\n",
            "--- Confusion Matrix ---\n",
            "[[965   0]\n",
            " [ 37 113]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precision = 1.00 (100%): This means Zero False Positives."
      ],
      "metadata": {
        "id": "87jDoH1WOdXl"
      }
    }
  ]
}