{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4FOU2ZUf6DLAYmolOwAMM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wezz-git/AI-samples/blob/main/Web_Scraping%20(S%26P%20500).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Business Problem: **\n",
        "\n",
        "You're a junior data scientist at a hedge fund. Your boss says, \"I need a list of all the companies in the S&P 500, their stock tickers, and what industry they're in. Find it.\" There's no API for this. The data is just sitting on a Wikipedia page.\n",
        "\n",
        "**This is Web Scraping:**\n",
        "\n",
        "You're going to write a script that \"reads\" a website's HTML source code and pulls the data out."
      ],
      "metadata": {
        "id": "K1NHG5gRapzM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KKMIsw9Rchnm"
      },
      "outputs": [],
      "source": [
        "# Tools:\n",
        "# requests: You already know this! We'll use it to download the webpage's raw HTML.\n",
        "# BeautifulSoup: This is the new library. It's a \"parser\" that makes it easy to navigate the messy HTML and find the exact table we need."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulSoup\n",
        "!pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EK2XhpaISBp0",
        "outputId": "a490c025-6b3e-4c71-e66a-7efb8ada2290"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting beautifulSoup\n",
            "  Downloading BeautifulSoup-3.2.2.tar.gz (32 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
        "\n",
        "# This \"disguise\" tells the server we are a normal browser\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
        "}\n",
        "\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "# We just check the .status_code\n",
        "# directly on the 'response' we already have.\n",
        "\n",
        "# Should get the status code: 200, meaning its OK\n",
        "print(f\"Status Code: {response.status_code}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_9xUvr4SaRC",
        "outputId": "60aef07f-da36-4926-ec90-84fe127035d5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status Code: 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 - import the new tool\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# 2 - create the 'soup'\n",
        "# This parses the raw HTML text we download\n",
        "\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "print(f\"Status code: {response.status_code}\")\n",
        "\n",
        "print(\"Successfully created 'soup' object. HTML is parsed and ready to be searched.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDM4hpMMU5Rz",
        "outputId": "b26f79a9-af53-4da3-f3b5-9e17ee60be60"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status code: 200\n",
            "Successfully created 'soup' object. HTML is parsed and ready to be searched.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Find the specific table ---\n",
        "# We're telling BeautifulSoup to find a 'table' tag\n",
        "# that has an id attribute of 'constituents'\n",
        "\n",
        "table = soup.find('table', id='constituents')\n",
        "\n",
        "# --- 2. Check if we found it ---\n",
        "\n",
        "if table:\n",
        "    print(\"Success! Found the S&P 500 constituents table.\")\n",
        "else:\n",
        "    print(\"Error: Could not find the table. The webpage structure might have changed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEPl0BVCXvgD",
        "outputId": "7ab3daf5-cca6-48d0-92ce-f295c1c70137"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success! Found the S&P 500 constituents table.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Convert the HTML table to a DataFrame\n",
        "\n",
        "# pd.read_html() is a powerful function that finds all tables.\n",
        "# We give it str(table) to tell it to read our *specific* table.\n",
        "# It returns a list of tables, so we get the first one [0].\n",
        "\n",
        "df_list = pd.read_html(str(table))\n",
        "df_sp500_df = df_list[0]\n",
        "\n",
        "# 2 - Check the new DataFrame\n",
        "\n",
        "print(\"Successfully converted the table to a DataFrame.\")\n",
        "print(df_sp500_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "e3VlkFPNYFQ7",
        "outputId": "621d2886-181f-420f-85c1-ac498b5ce8c3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully converted the table to a DataFrame.\n",
            "  Symbol             Security             GICS Sector  \\\n",
            "0    MMM                   3M             Industrials   \n",
            "1    AOS          A. O. Smith             Industrials   \n",
            "2    ABT  Abbott Laboratories             Health Care   \n",
            "3   ABBV               AbbVie             Health Care   \n",
            "4    ACN            Accenture  Information Technology   \n",
            "\n",
            "                GICS Sub-Industry    Headquarters Location  Date added  \\\n",
            "0        Industrial Conglomerates    Saint Paul, Minnesota  1957-03-04   \n",
            "1               Building Products     Milwaukee, Wisconsin  2017-07-26   \n",
            "2           Health Care Equipment  North Chicago, Illinois  1957-03-04   \n",
            "3                   Biotechnology  North Chicago, Illinois  2012-12-31   \n",
            "4  IT Consulting & Other Services          Dublin, Ireland  2011-07-06   \n",
            "\n",
            "       CIK      Founded  \n",
            "0    66740         1902  \n",
            "1    91142         1916  \n",
            "2     1800         1888  \n",
            "3  1551152  2013 (1888)  \n",
            "4  1467373         1989  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-14965421.py:9: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  df_list = pd.read_html(str(table))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 - clean the data (good for practice)\n",
        "# Want to get the 'Symbol', 'Security' and 'GICS Sector'\n",
        "\n",
        "final_df = df_sp500_df[['Symbol', 'Security', 'GICS Sector']]\n",
        "\n",
        "print(\"\\n-- Final, Cleaned list --\")\n",
        "print(\"Successfully cleaned the data.\")\n",
        "print(final_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZOggYwCZM0U",
        "outputId": "5f3e2d57-9833-4eae-c3cf-46fcb85ac172"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- Final, Cleaned list --\n",
            "Successfully cleaned the data.\n",
            "  Symbol             Security             GICS Sector\n",
            "0    MMM                   3M             Industrials\n",
            "1    AOS          A. O. Smith             Industrials\n",
            "2    ABT  Abbott Laboratories             Health Care\n",
            "3   ABBV               AbbVie             Health Care\n",
            "4    ACN            Accenture  Information Technology\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ran the entire web scraping workflow:\n",
        "\n",
        "1 - Downloaded the page (handling 403 errors).\n",
        "\n",
        "2 - Parsed the messy HTML (BeautifulSoup).\n",
        "\n",
        "3 - Located the exact data you needed (soup.find).\n",
        "\n",
        "4 - Extracted and cleaned the data into a usable format (pd.read_html).\n",
        "\n",
        "Now have a DataFrame with all 500 companies and their tickers."
      ],
      "metadata": {
        "id": "Ni9mRpsBaTgT"
      }
    }
  ]
}